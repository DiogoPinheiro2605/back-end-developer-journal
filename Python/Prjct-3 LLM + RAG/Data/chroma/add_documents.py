import os
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import DirectoryLoader, TextLoader # Importa TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter 
from langchain_ollama import OllamaEmbeddings
from typing import List

# üí• CONFIGURA√á√ÉO DE CAMINHOS ABSOLUTOS
# Isto aponta para o diret√≥rio onde o script est√°: .../Data/chroma
BASE_DIR = os.path.dirname(os.path.abspath(__file__)) 

# 1. Diret√≥rio de ORIGEM dos dados (Data/knowledge)
# √â preciso subir um n√≠vel (..) de 'chroma/' e entrar em 'knowledge/'
DATA_DIR = os.path.join(BASE_DIR, "..", "knowledge") 

# 2. Diret√≥rio de PERSIST√äNCIA do Chroma (Data/chroma/chroma_db)
CHROMA_DIR = os.path.join(BASE_DIR, "chroma_db") 
os.makedirs(CHROMA_DIR, exist_ok=True) 

# üîÑ CONFIGURA√á√ÉO DE EMBEDDINGS
embeddings = OllamaEmbeddings(model="mxbai-embed-large")

# --- FUN√á√ïES DE CARREGAMENTO E PROCESSAMENTO ---

def load_documents_from_directory(directory_path: str) -> List[Document]:
    """
    Carrega documentos de um diret√≥rio usando TextLoader para evitar a depend√™ncia 'unstructured'.
    Assume que os ficheiros s√£o .txt
    """
    print(f"üîÑ A carregar ficheiros do diret√≥rio: {directory_path}...")
    
    if not os.path.exists(directory_path):
        print(f"‚ùå ERRO: Diret√≥rio n√£o encontrado: '{directory_path}'")
        return []
    
    # Usar TextLoader para ficheiros .txt evita a depend√™ncia 'unstructured'
    loader = DirectoryLoader(
        path=directory_path,
        glob="**/*.txt",           # Apenas carrega ficheiros .txt
        show_progress=True,
        loader_cls=TextLoader,     # For√ßa o uso do TextLoader simples
        loader_kwargs={"encoding": "utf-8"} 
    )
    
    documents = loader.load()
    print(f"‚úÖ Carregados {len(documents)} documentos.")
    return documents

def split_documents(documents: List[Document]) -> List[Document]:
    """Divide os documentos em chunks mais pequenos."""
    print("üîÑ A dividir documentos em chunks...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=200,
        separators=["\n\n", "\n", " ", ""]
    )
    chunks = text_splitter.split_documents(documents)
    print(f"‚úÖ Dividido em {len(chunks)} chunks.")
    return chunks

def get_or_create_vectordb(collection_name: str) -> Chroma:
    """Carrega uma cole√ß√£o Chroma existente ou cria uma nova, garantindo persist√™ncia."""
    persist_dir = os.path.join(CHROMA_DIR, collection_name)
    os.makedirs(persist_dir, exist_ok=True)
    
    print(f"\nüîÑ Tentando carregar/criar cole√ß√£o '{collection_name}' em {persist_dir}...")
    
    vectordb = Chroma(
        persist_directory=persist_dir,
        embedding_function=embeddings,
        collection_name=collection_name
    )
    
    doc_count = vectordb._collection.count()
    print(f"‚úÖ Cole√ß√£o pronta. Documentos existentes: {doc_count}")
    return vectordb

def add_documents_to_collection(collection_name: str, documents: List[Document]):
    """Adiciona documentos processados √† cole√ß√£o Chroma, com verifica√ß√£o de duplica√ß√£o."""
    
    vectordb = get_or_create_vectordb(collection_name)
    
    # Adicionar apenas se for uma primeira execu√ß√£o (Verifica√ß√£o Anti-Duplica√ß√£o Simples)
    initial_count = vectordb._collection.count()
    
    if initial_count == 0:
        vectordb.add_documents(documents)
        
        final_count = vectordb._collection.count()
        print(f"‚úÖ Cole√ß√£o '{collection_name}' inicializada! Adicionados {len(documents)} chunks.")
    else:
        print(f"‚ÑπÔ∏è Cole√ß√£o '{collection_name}' j√° cont√©m {initial_count} chunks. Nada adicionado para evitar duplica√ß√£o.")

# --- FLUXO PRINCIPAL ---
if __name__ == "__main__":
    
    # üí• DEFINA A LISTA DE PASTAS QUE QUER PROCESSAR AQUI
    # Cada nome deve corresponder a uma subpasta em Data/knowledge/
    collections_to_process = ["estrategias_venda", "marketing"] 
    
    for collection_name in collections_to_process:
        
        # O caminho de origem √©: Data/knowledge/{collection_name}
        target_data_path = os.path.join(DATA_DIR, collection_name)
        
        # 1. Carregar documentos (lida com m√∫ltiplos ficheiros .txt)
        all_documents = load_documents_from_directory(target_data_path)
        
        if all_documents:
            # 2. Dividir documentos em chunks
            all_chunks = split_documents(all_documents)
            
            # 3. Adicionar chunks √† Chroma DB
            add_documents_to_collection(collection_name, all_chunks)
        else:
            print(f"‚ö†Ô∏è Ignorando cole√ß√£o '{collection_name}'. N√£o foram encontrados documentos .txt.")